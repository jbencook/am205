\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
%\usepackage{hyperref}
\usepackage[round]{natbib}
\usepackage{url}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{comment}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09

\title{Incremental SVD for large-scale recommendation algorithms}

\author{
Stephen Merity
\And
J. Benjamin Cook
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\T}{\textrm{T}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
Singular value decomposition (SVD) is fundamental to many machine learning algorithms, primarily being used for dimensionality reduction.
Unfortunately, SVD algorithms typically have cubic complexity and require random access to the complete data set, making them impractical for many real world tasks.
We investigate the properties of incremental singular value decomposition which does not require the complete data set to be stored in memory.
We then compare this with a traditional, full SVD algorithm to see the impact on orthogonality of the resulting SVD.
To motivate incremental SVD in the context of machine learning, we construct a recommendation system and show the performance of SVD compared to incremental SVD, showing the potential to incrementally build SVD-based models and produce a highly scalable recommendation system.
\end{abstract}

\section{Introduction}

Recommendation systems aim to recommend an item to interested potential customers, tailoring the choice of item based upon both that customer's history and the history of other users.
The items recommended can be diverse, including movies, music, web pages, or other general products.
These systems have evolved from novelties to vital tools that are re-shaping the world of e-commerce.
By learning from both the customer and the broader community as to which items should be recommended to someone, these successful recommendations can lead to substantial improvements in both revenue and customer satisfaction \citep{schafer1999recommender}.

Within recommendation systems, there are two primary types: \textit{content-based} recommenders and \textit{collaborative filtering} recommenders.
Content-based approaches analyze the content, such as the text, metadata, or features of an item, to identify related items.
A known successful realization of content-based recommendation is music recommendation on services such as Pandora or Spotify.
Hundreds of features are created for each song, either manually or automatically.
These features aim to capture the significant characteristics of the piece of music, allowing recommendations for a user either explicitly through their own preferences (the user stating they like jazz) or implicitly through the user's past behaviour.

An alternative approach, which is content agnostic, is collaborative filtering.
Collaborative filtering analyzes the relationships between user choices within a given community to make recommendations.
A major appeal of this method is that it is domain free and does not need any information about the item being recommended.
Though it does suffer from the \textit{cold start} problem\footnote{The \textit{cold start} problem refers to the issue of making recommendations before a large enough history of user behaviour is available.}, given large amounts of user history, collaborative filtering is the most popular method for recommendation systems.
Collaborative filtering is widely deployed across the Internet, seeing specific popularity after the introduction of the Netflix prize competition in 2006.

When given a ratings matrix for a collaborative filtering task, it is important to recognize that there is both signal and noise.
The signal would be the underlying customer's true rating of the item.
The noise could arise from factors such as watching a film on a bad day, misclicking the ratings widget, or a user rating an item on someone else's account.
To get a good approximation of the missing ratings, we need to remove the noise and use the recovered signal to predict missing ratings.

Many of the methods for collaborative filtering either use singular value decomposition (SVD) or are SVD inspired.
SVD works well in this task as it has the important property of providing the best low-rank approximation of the original matrix, as determined by the Frobenius norm.
When the SVD minimizes the Frobenius norm, this is equivalent to minimizing the root mean square error (RMSE) over all elements of the matrix.
This dimensionality reduction approach intuitively maps to the collaborative filtering process by mapping customers who rate products similarly into the space spanned by the same eigenvectors.
These represent latent features which in the Netflix process commonly map to movie genres.
By taking only a low-rank approximation of this, we can filter out small singular values that represent noise in the rating process.

It should be noted that for real world recommendation systems, variations of SVD are used.
These variations handle the sparsity of the ratings matrix more effectively \citep{kurucz2007methods}, add regularization to prevent overfitting \citep{zhou2008large}, and merge multiple methods and information about the problem to improve the recommendations \citep{bell2007bellkor}.

In this paper, we give an introduction to collaborative filtering on large datasets using incremental SVD.

\section{Data}

The Netflix Prize was a large-scale recommendation competition held by Netflix.
Their aim was to improve the recommendations they provided for their users by allowing third party researchers to analyze their data.
At the time, the Netflix dataset was the largest real world dataset available to researchers.
Collected over 7 years, it contained over 100 million ratings for 17,700 movies provided by over 480,000 users.
To compete, participants would send predicted ratings for a specific test set to Netflix.
Netflix would then return the root mean squared error (RMSE) for a portion of this test set.
By providing RMSE on only a portion of the test set, teams cannot overfit the dataset to win the competition as their accuracy on the hidden portion would fall substantially.
After the competition concluded, this dataset was released publicly for continued research.
A full description of the rules and dataset can be found at the Netflix Prize website.


%Here, we talk about the Netflix dataset. How we scrubbed it, what it consists of, etc.
The Netflix dataset consists of 17,770 text files.
Each text file represents a distinct movie.
The first line in the text file is the movie's unique ID number, which is an integer from 1 to 17,770.
All other lines have three comma-delimited entries: user ID, rating, and date.

There are 480,189 unique users in the dataset, with their IDs ranging from 1 to 2,649,429, with gaps.
Ratings are integers from one to five indicating the number of stars the user gave to the movie in question.
Dates are in the YYYY-MM-DD format, although we do not use this information in the current project.
For example:

\begin{verbatim}
1:
1488844,3,2005-09-06
822109,5,2005-05-13
885013,4,2005-10-19
30878,4,2005-12-26
...
\end{verbatim}

The above data come from the first movie's text file, which contains ratings for the movie \emph{Dinosaur Planet}. The data indicate that user 1488844 gave \emph{Dinosaur Planet} a three star rating on September 6, 2005, user 822109 gave the movie a five star rating on May 13, 2005, and so on.

In order to be able to perform SVD, we need a matrix with users on the rows and movies on the columns.
This matrix would be $480,179 \times 17,770 = 8.5 \textrm{ billion}$ entries.
In a regular matrix format, this would too big to hold in memory.
One estimate is that it takes roughly 65 GB of RAM to hold the entire matrix \citep{revoR} although the actual size would depend on the amount of space allocated for each rating.
Fortunately, the matrix is extremely sparse, containing around 100 million non-zero entries.
To store the data in our project, we use SciPy's \verb!scipy.sparse.lil_matrix! which constructs sparse matrices using row-based linked lists.
We store data from the text files in this sparse matrix as we read them.
After reading in all of the text files, we output the matrix to a Matrix Market format.
The Matrix Market format starts with a line containing the dimensions of the matrix and the number of non-zero entries.
Then, each line contains $i \enskip j \enskip rating$.
For example, these are the first few lines of a Matrix Market file with a subset of the Netflix data:

\begin{verbatim}
20000 1000 564726
1 1 3
1 8 4
1 17 2
1 30 3
...
\end{verbatim}

Finally, because the process of implementing our incremental SVD system was iterative and because even the iterative method requires serious computational power, we reduced our dataset to smaller subsets for testing.
We ran our algorithm on datasets of size $3000 \times 1000$ and $3000 \times 3000$.

\section{Method}

In the context of recommendation systems, the main task of Singular Value Decomposition (SVD) is to decrease the dimensionality of the dataset.
We need to be able to summarize the key characteristics of a movie in a much smaller number of features, or variables, than the total number of users in the system.
Similarly, we need to reduce the key preferences of users to something much smaller than the total number of movies in the database.
The reasons for decreasing the dimensionality of the dataset are two-fold.
First, without dimensionality reduction, machine learning tasks would be computationally intractable.
Second, reducing the dimensionality actually allows us to predict ratings more effectively, since our dataset is so sparse.

\subsection{SVD}

As with other matrix factorization techniques, SVD works by decomposing a matrix, $\A \in \mathbb{R}^{m \times n}$, where $m \geq n$ and
$\textrm{rank}(\A) = r$,
into separate matrices whose product is $\A$:
$$\A = \mathbf{U}\mathbf{S}\mathbf{V}^{\T}$$
Here, $\mathbf{U} \in \mathbb{R}^{m \times r}$, is composed of the eigenvectors of $\A\A^{\T}$, or left-singular vectors of $\A$, 
$\mathbf{S} \in \mathbb{R}^{r \times r}$ is a diagonal matrix whose elements are the $r$ singular values of $\A$,
and $\mathbf{V}^{\T} \in \mathbb{R}^{r \times n}$ is composed of the
eigenvectors of $\A^{\T}\A$, or right-singular vectors of $\A$
\citep{golub1970}.
Furthermore, the rows and columns of $\mathbf{U}$, $\mathbf{S}$, and $\mathbf{V}^{\T}$ are sorted in such a way that the largest singular values occur in the upper left most corner of $\mathbf{S}$.
This means that we can achieve a low-rank approximation to $\A$ by considering only the $k$ first singular values.

\begin{align*}
	\A &\approx \A_k\\
	&= \mathbf{U}_k\mathbf{S}_k\mathbf{V}_k^{\T}\\
	&= \mathbf{U}[:,1:k]\mathbf{S}[1:k,1:k]\mathbf{V}[:,1:k]^{\T}
\end{align*}

This approximation $\A_k$ is the rank $k$ matrix that minimizes the Frobenius norm: $\|\A - \A_k\|_{\textrm{F}}$.

Although we attempted to implement the full SVD algorithm by following the recipe laid out in \cite{recipes2007}, it quickly became apparent that implementing SVD from scratch was beyond the scope of this project.
Furthermore, due to the large time complexity of the algorithm, an implementation in pure Python would be too slow for practical use with this dataset.

\subsection{Folding-in}
The SVD algorithm requires $O(m^3)$ time complexity.
On the Netflix training dataset with 480,179 users, this is on the order of $10^{17}$ operations, which is infeasible without a super computer.
Fortunately, a technique called folding-in allows us to compute SVD on a subset of users (or movies) and then add users (movies) incrementally.

This approximation of $\A$ becomes worse as we fold-in more and more users, and we lose orthogonality.
In the context of recommendation systems, however, the loss of orthogonality is not necessarily a core concern.
The approximation produced by the incremental SVD algorithm performs well enough to predict user ratings to within a tolerable amount of error, especially as the exact initial values are already unknown.
Additionally, each user can be folded-in in $O(1)$ time, meaning it is possible to approximate SVD of large matrices on a standard laptop.

Assuming we want to fold-in users, we have two parameters, the number of singular values to use, $k$, and the number of users to begin with, $u$.
The procedure described in \cite{slides} states that to compute the incremental SVD of a matrix:

\begin{enumerate}
\item Compute full SVD for the first $u$ users:
$$\textrm{SVD}(\A[1:u,:]) = \mathbf{U}\mathbf{S}\mathbf{V}^{\T}$$
\item Take the first $k$ singular values:
$$\A_k= \mathbf{U}_k\mathbf{S}_k\mathbf{V}_k^{\T}$$
\item For $i$ in $u+1$ to $m$:
\begin{align*}
&c = \A[i,:]\\
&c^{\prime} = c \mathbf{V}_k \mathbf{S}_k^{-1}\\
&\textrm{Append $c^{\prime}$ to the bottom of $\mathbf{U}_k$}
\end{align*}
\end{enumerate}

In order to fold-in movies instead of users, we replace the parameter $u$ with $v$, the number of movies to begin with. Then step one becomes:

$$\textrm{SVD}(\A[:,1:v]) = \mathbf{U}\mathbf{S}\mathbf{V}^{\T}$$

And for step three, we repeat the following for $j$ in $v+1$ to $n$:

\begin{align*}
&p = \A[:,j]\\
&p^{\prime} = p^{\T}\mathbf{U}_k\mathbf{S}_k^{-1}\\
&\textrm{Append $p^{\prime}$ to the right side of $\mathbf{V}_k^{\T}$}
\end{align*}

\subsection{Predicting ratings}

After folding-in all users and movies, it is possible to use $\mathbf{U}_k$, $\mathbf{S}_k$, and $\mathbf{V}_k^{\T}$ to predict user ratings that we have not observed, i.e. empty elements in our ratings matrix.
For example, to estimate the rating that user $i$ would give movie $j$, we simply take:

$$P_{i,j} = \bar{r}_i + \mathbf{U}\sqrt{\mathbf{S}_k}^{\T}[i,:]
\cdot \sqrt{\mathbf{S}_k}\mathbf{V}_k^{\T}[:,j]$$

where $\bar{r}_i$ is the row mean of the non-zero elements of $\A$ \citep{sarwar2002}.

\section{Results}

%Explain how we tested predictions
%Explain how we tested orthogonality
We evaluate incremental SVD in three ways.
First, we hold out a test set of ratings and use our recommendation system to predict their values.
A popular measure for the performance of incremental SVD is root mean squared error, $\sqrt{\frac{\sum_{i=1}^N \left(y_i - \hat{y}_i\right)^2}{N}}$ where $y$ is the vector of user ratings and $\hat{y}$ is the vector of predictions for the ratings in the test set.
Next, we assess the `accuracy' of our approximation of $\A$, $\|\A - \A_k\|_{\textrm{F}}$ where $\A_k$ is constructed from the $\mathbf{U}_k$ that is built with folding-in.
Finally, in full SVD, the columns of $\mathbf{U}$ are orthogonal, meaning $\mathbf{U}\mathbf{U}^{\T} = \mathbb{I}_m$.
We assess the deviation from orthogonality by computing $\|\mathbf{U}\mathbf{U}^{\T} - \mathbb{I}_m\|_{\textrm{F}}$.

In all the plots below, the value of $k$ represents the low-rank approximation to the matrix generated by considering only the $k$ first singular values, and the value of $u$ represents the $u \times u$ matrix used to initialize the incremental SVD algorithm.

We first investigated the impact of incremental SVD on reconstructing the original matrix.
In Figure \ref{fig:recon_small}, we investigate the error in reconstructing the ratings matrix $\A$ when using incremental SVD compared to traditional SVD.
The SVD represents the best low-rank approximation of the original matrix, and is hence a lower-bound for the incremental SVD algorithm.
We tested incremental SVD for multiple values of $u$ and $k$ and found that the reconstruction error continues to decline at a similar rate to that of traditional SVD for $k < 100$.
This holds promise as it suggests incremental SVD may be useful in creating recommendation systems even when the values of $u$ are substantially smaller than the original dataset size.
%This would be especially true if we could create a higher low-rank
%Even for only a small value of $u$, the reconstruction error .
Unsurprisingly, large values of $k$ continue to get us closer to the original matrix, and this holds true for both SVD and incremental SVD.

\begin{figure}[H]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\includegraphics[width=1\textwidth]{../images/reconstruct_fro_3000x1000}}
%\fbox{\includegraphics{../scatter_results}}
\end{center}
\caption{Reconstruction error
$\|\mathbf{U}_k\mathbf{S}_k\mathbf{V}_k^{\T} - \A\|_{\textrm{F}}$ for
the $3,000 \times 1,000$ ratings matrix.}
\label{fig:recon_small}
\end{figure}

\newpage

In Figure \ref{fig:ortho_small}, the deviations from orthogonality, 
$\|\mathbf{U}_k\mathbf{U}_k^{\T}\|_2$ are plotted as $k$ increases for several values of $u$.
As with Figure \ref{fig:error_small}, the line where $u = 3,000$ represents SVD on the entire ratings matrix.
The full SVD on the ratings matrix has zero orthogonality error.
For the results of the incremental SVD algorithm, deviations from orthogonality are much more pronounced when starting with smaller subsets of the data (i.e. lower values of $u$).
As the value of $k$ increases, the deviation gets worse as each row added by the incremental SVD algorithm can add more fluctuations.
For larger values of $u$, however, the deviation from orthogonality increases slowly as $k$ increases.

\begin{figure}[H]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\includegraphics[width=1.0\textwidth]{../images/ORTHO}}
%\fbox{\includegraphics{../scatter_results}}
\end{center}
\caption{Deviations from orthogonality, 
$\|\mathbf{U}_k\mathbf{U}_k^{\T} - \mathbb{I}_m\|_{\textrm{F}}$, for
incremental SVD starting with several different numbers of users.}
\label{fig:ortho_small}
\end{figure}

In Figure \ref{fig:error_small}, we show the root mean squared error for our prediction of user ratings on the test set as $k$ increases.
We ran our prediction beginning with several different numbers of users $u$ (displayed in the legend).
Note that $u = 3,000$ is SVD on the entire ratings matrix.
As mentioned previously, real world recommendation systems use modified versions of the SVD algorithm for recommendations.
Modified versions are required as the traditional SVD algorithm accurately reconstructs the sparse matrix, specifying that the majority of the entries should be zero.
As the resulting output from the incremental SVD is unlikely to accurately minimize the variations, leaving many of the entries that would be zero from the full SVD result as non-zero, the less accurate incremental SVD results are actually better.
It is due to this that we see the incremental SVD system with low values for $u$ outperforming the full SVD algorithm.

Given this, however, we can see that the curves for $u=3000$ (full SVD) and $u=2000$ (the largest incremental SVD result) are quite similar in behaviour.
The curves also all have similar behaviour as the value of $k$ increases.


\begin{figure}[H]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\includegraphics[width=1\textwidth]{../images/rmse_3000x1000}}
%\fbox{\includegraphics{../scatter_results}}
\end{center}
\caption{RMSE as $k$ increases for several values of $u$ on a 3,000 $\times$ 1,000 subset of the ratings matrix. 10\% of the ratings are held out for the test set.}
\label{fig:error_small}
\end{figure}


\section{Conclusion}
In this paper, we implemented incremental SVD and used it to construct a simple recommendation system.
This recommendation system and the components it used were analyzed and the performance of the system was measured against a subset of the ratings matrix used in the Netflix Prize competition.
While incremental SVD does not retain all the properties of typical SVD, such as orthogonality or exact reconstruction of the original matrix, we have shown that it can still be used effectively as the basis of a recommendation system.
By using incremental SVD, recommendation systems can scale up to substantially larger data sets while using only a small portion of the computing power required by methods utilizing traditional SVD.

\bibliography{merity_cook}
\bibliographystyle{plainnat}

\end{document}
