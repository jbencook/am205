\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
%\usepackage{hyperref}
\usepackage[round]{natbib}
\usepackage{url}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{comment}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09

\title{Predicting user ratings with incremental SVD}

\author{
Stephen Merity
\And
J. Benjamin Cook
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\T}{\textrm{T}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
Hello, World. This is an abstract.
\end{abstract}

\section{Introduction}
In this section, we give an introduction to our incremental SVD paper.

\section{Dataset}
%Here, we talk about the Netflix dataset. How we scrubbed it, what it consists of, etc.
The core of the Netflix dataset consists of 17,770 text files.
Each text file represents a distinct movie.
The first line in the text file is the movie's unique ID number, which is
an integer from 1 to 17,770.
All other lines have three comma-delimited entries: user ID, rating, and date.

There are 480,189 unique users in the dataset, however, IDs range from 1 to 2,649,429, with gaps. 
Ratings are integers from one to five indicating the number of stars 
the user gave to the movie in question.
Dates are in the YYYY-MM-DD format, although we do not use this information in the current project. 
For example:
\begin{verbatim}
1:
1488844,3,2005-09-06
822109,5,2005-05-13
885013,4,2005-10-19
30878,4,2005-12-26
...
\end{verbatim}
The above data come from the first movie's text file, which contains ratings for the movie \emph{Dinosaur Planet}. The data indicate that user 1488844 gave \emph{Dinosaur Planet} a three star rating on September 6, 2005, user 822109 gave the movie a five star rating on May 13, 2005, and so on.

In order to be able to perform SVD, we need a matrix with users on the rows and movies on the columns.
This matrix would be $480,179 \times 17,770 = 8.5 \textrm{ billion}$ entries.
In a regular matrix format, this would too big to hold in memory. One estimate is that it takes roughly 65 Gb of RAM to hold the entire matrix \citep{revoR} although the actual size would depend on the size of the amount of space allocated for each rating.
Fortunately, the matrix is extremely sparse, containing only around 100 million non-zero entries.
The data structure we use is Python's \verb!scipy.sparse.lil_matrix!.
We store data from the text files in this sparse matrix as we read them.
After reading in all of the text files, we output the matrix to a Matrix Market format.
The Matrix Market format starts with a line containing the dimensions of the matrix and the number of non-zero entries.
Then, each line contains $i \hspace{2ex} j \hspace{2ex} <\textrm{value}>$.
For example, these are the first few lines of a Matrix Market file with a subset of the Netflix data :
\begin{verbatim}
20000 1000 564726
1 1 3
1 8 4
1 17 2
1 30 3
...
\end{verbatim}

Finally, because the process of implementing our incremental SVD system was iterative and because even the iterative method requires serious computational power, we reduced our dataset to smaller subsets for testing.
We ran our algorithm on datasets of size $1000 \times 2000$, $1000 \times 3000$, \ldots.

\section{Method}
In the context of recommender systems, the main task of Singular Value Decomposition (SVD) is to decrease the dimensionality of the dataset.
We need to be able to summarize the key characteristics of a movie in a much smaller number of features, or variables, than the total number of users in the system.
Similarly, we need to reduce the key preferences of users to something much smaller than the total number of movies in the database.
The reasons for decreasing the dimensionality of the dataset are two-fold.
First, without dimensionality reduction, machine learning tasks would be computationally intractable and second, reducing the dimensionality actually allows us to predict ratings more effectively, since our dataset is so sparse.

\subsection{SVD}
As with other matrix factorization techniques, SVD works by decomposing a matrix, $\A \in \mathcal{R}^{m \times n}$, where $m \geq n$ and 
$\textrm{rank}(\A) = r$, 
into separate matrices whose product is $\A$:
$$\A = \mathbf{U}\mathbf{S}\mathbf{V}^{\T}$$
Here, $\mathbf{U} \in \mathcal{R}^{m \times r}$, is composed of the eigenvectors of $\A\A^{\T}$, 
$\mathbf{S} \in \mathcal{R}^{r \times r}$ is a diagonal matrix whose elements are the $r$ singular values of $\A$,
and $\mathbf{V}^{\T} \in \mathcal{R}^{r \times n}$ is composed of the
eigenvectors of $\A^{\T}\A$ \cite{golub1970}.
Furthermore, the rows and columns of $\mathbf{U}$, $\mathbf{S}$, and $\mathbf{V}^{\T}$ are sorted in such a way that the largest singular values occur in the upper left most corner of $\mathbf{S}$.
This means that we can achieve a low-rank approximation to $\A$ by considering taking only the $k$ first singular values.

\begin{align*}
	\A &\approx \A_k\\
	&= \mathbf{U}_k\mathbf{S}_k\mathbf{V}_k^{\T}\\
	&= \mathbf{U}[:,1:k]\mathbf{S}[1:k,1:k]\mathbf{V}[1:k,:]^{\T}
\end{align*}

This approximation $\A_k$ is the rank $k$ matrix that minimizes the Frobenius norm: $\|\A - \A_k\|_{\textrm{F}}$.

Although we attempted to implement the full SVD algorithm following the recipe laid out in \cite{recipes2007}, it quickly became clear that implementing SVD from scratch was beyond the scope of this project.

\subsection{Predicting Ratings}

\subsection{Folding-in}

\section{Results}
These are our results. They are pretty darn good.

\begin{comment}
\begin{figure}[H]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\includegraphics[width=.9\textwidth]{scatter_results}}
%\fbox{\includegraphics{../scatter_results}}
\end{center}
\caption{Performance of the edge weight inference algorithm for a graph with $|V| = 100$ and $|E|= 348$}
\label{fig:scatter1}
\end{figure}
\end{comment}

\section{Discussion}
This is a discussion of how well we did.

\section{Conclusion}
In conlusion, this paper was awesome. Cheers.

\bibliography{merity_cook}
\bibliographystyle{plainnat}

\end{document}